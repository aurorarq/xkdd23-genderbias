## Exploring gender bias in misclassification with clustering and local explanations

This repository contains the replication package of the paper "Exploring gender bias in misclassification with clustering and local explanations", accepted for presentation in the [XKDD workshop](http://xkdd2023.isti.cnr.it/) colocated with [ECML 2023](https://2023.ecmlpkdd.org/).

### Repository organisation

The repository is organised as follows:

- [code](https://github.com/aurorarq/xkdd23-genderbias/tree/main/code): Python scripts to run the experiments.
- [data](https://github.com/aurorarq/xkdd23-genderbias/tree/main/data): Datasets in CSV format.
- [results](https://github.com/aurorarq/xkdd23-genderbias/tree/main/results): CSV files with the statistics and results for the two research questions.

### Dependencies

The code has been developed with Python 3.10.2 using Visual Code Studio. Machine learning algorithms are built with [sklearn](https://github.com/scikit-learn/scikit-learn) and [fairlearn](https://github.com/fairlearn/fairlearn). Explanations are generated by [dalex](https://github.com/ModelOriented/DALEX/).

To run the experiments, follow these steps:

1. Clone/zip this repository
2. Create a virtual environment using python env/conda. For python env: `python -m venv <your-venv-path>`
3. Activate your virtual environment (activate script on /bin or /Scripts depending on your OS)
4. Install the dependencies from the requirements file. For pip: `pip install -r requirements.txt`
5. Go to the code folder and run the desired script, e.g., `python experiment_adult_all_data.py`

### Datasets

The experiments use three datasets:

- Dataset 1: adult income, the original file is available on [kaggle](https://www.kaggle.com/datasets/wenruliu/adult-income-dataset).
- Dataset 2: dutch census, a preprocessed file is available on [github](https://github.com/alku7660/counterfactual-fairness/blob/main/Datasets/dutch/preprocessed_dutch.csv).
- Dataset 3: employee promotion, the original file is available on [kaggle](https://www.kaggle.com/datasets/arashnic/hr-ana).

### Experiments

For each dataset, five different models are built as described in the paper: 

- "Full" model: Baseline classifier with all attributes used for training.
- "No gender" model: A classifier trained without the gender attribute.
- "One gender" model: A classifier trained with data separated by gender.
- "Mit_in" model: A classifier with in-processing bias mitigation method (ExponentiatedGradient)
- "Mit_post" model: A classifier with post-processing bias mitigation method (ThresholdOptimizer).

The five strategies are applied to two different classification algorithms: Random Forest and Gradient Boosting Tree. After training, the instances misclassified by the model are analysed as follows:

- Clustering (Affinity Propagation algorithm) is executed to find groups of similar missclassified instances, separated by false positives/false negatives and gender (RQ1).
- For each cluster, a prototype instance is analysed using Break-down to discover the most relevant features causing the wrong prediction (RQ2).

### Funding

This work has been developed as part of the [GENIA project](https://github.com/aurorarq/genia), funded by the Annual Research Plan (2022) of the University of CÃ³rdoba (Spain).

